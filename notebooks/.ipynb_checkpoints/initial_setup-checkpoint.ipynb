{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1512c4-fe75-4115-bb82-91f510648bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /proj/src/Py already exists. No files were created.\n",
      "Directory /proj/run/Py already exists. No files were created.\n",
      "Directory /proj/inputs already exists. No files were created.\n",
      "Directory /proj/outputs already exists. No files were created.\n"
     ]
    }
   ],
   "source": [
    "# set a directory structure\n",
    "\n",
    "import os, json  # Import the os module to interact with the operating system\n",
    "\n",
    "def create_directory_structure(root_dir):\n",
    "    # Define paths for src, run, inputs, and outputs\n",
    "    src_at = os.path.join(root_dir, \"src\", \"Py\")  # Path for the source directory\n",
    "    run_at = os.path.join(root_dir, \"run\", \"Py\")  # Path for the run directory\n",
    "    inputs_at = os.path.join(root_dir, \"inputs\")  # Path for the inputs directory\n",
    "    outputs_at = os.path.join(root_dir, \"outputs\")  # New output directory\n",
    "\n",
    "    # Create a dictionary to define directories and their initial files\n",
    "    directories_and_files = {\n",
    "        src_at: {  # Source directory with initial files\n",
    "            '__init__.py': \"# Sample package initialization\\n\",  # Package init file\n",
    "            'libs.py': \"# Add common libraries here to be loaded for all analysis\\n\",  # Placeholder for common libraries\n",
    "            'func.py': \"# Add custom functions here to be loaded for all analysis\\n\"  # Placeholder for custom functions\n",
    "        },\n",
    "        run_at: {  # Run directory with a main script\n",
    "            'dodo.py': \"#!/usr/bin/env python3\"  # Shebang for executing the script\n",
    "        },\n",
    "        inputs_at: {},  # Inputs directory (no initial files)\n",
    "        outputs_at: {}  # Outputs directory (no initial files)\n",
    "    }\n",
    "\n",
    "    created_directories = {}  # Initialize a dictionary to track created directories\n",
    "\n",
    "    # Iterate through the directories and their associated files\n",
    "    for directory, files in directories_and_files.items():\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)  # Create the directory\n",
    "            # Store the absolute path of the created directory with a modified key\n",
    "            created_directories[directory.replace(root_dir + '/', '').replace('/', '_')] = os.path.abspath(directory)  \n",
    "            # Create and write the specified files in the new directory\n",
    "            for file_name, content in files.items():\n",
    "                with open(os.path.join(directory, file_name), 'w') as f:  # Open a new file for writing\n",
    "                    f.write(content)  # Write the initial content to the file\n",
    "        else:\n",
    "            print(f\"Directory {directory} already exists. No files were created.\")  # Inform if the directory already exists\n",
    "            # Store the absolute path of the existing directory with a modified key\n",
    "            created_directories[directory.replace(root_dir + '/', '').replace('/', '_')] = os.path.abspath(directory)  \n",
    "\n",
    "    return created_directories  # Return the dictionary of created or existing directories\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = '/proj'\n",
    "directory_structure = create_directory_structure(root_dir) \n",
    "\n",
    "def write_json(data, path, verbose = False):\n",
    "    with open(path, \"w\") as fp:   \n",
    "        json.dump(data, fp)\n",
    "    if verbose:\n",
    "        return print(\"Done\")\n",
    "\n",
    "write_json(directory_structure, \"/proj/inputs/core_paths.json\") # saves the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183ccfe4-2df4-4e6a-a5f7-b3e3d3d1b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cv file\n",
    "import pandas as pd\n",
    "p_data = pd.read_feather(\"/proj/ext_dir/data_ravi/14_10_2024/example_pheno.feather\")\n",
    "\n",
    "# Assign data types\n",
    "p_data[\"trait\"] = p_data[\"trait\"].astype(\"str\")\n",
    "p_data[\"genotype\"] = p_data[\"genotype\"].astype(\"str\")\n",
    "p_data[\"blups\"] = p_data[\"blups\"].astype(\"float64\")\n",
    "p_data[\"std.error\"] =p_data[\"std.error\"].astype(\"float64\")\n",
    "p_data[\"dataset\"] = p_data[\"dataset\"].astype(\"str\")\n",
    "p_data[\"type\"] = p_data[\"type\"].astype(\"str\")\n",
    "#data.dtypes # check column data types\n",
    "\n",
    "# Check overlap\n",
    "test = p_data.loc[p_data.type == \"test\", \"genotype\"].to_list()\n",
    "train = p_data.loc[p_data.type == \"train\", \"genotype\"].to_list()\n",
    "[print(x)for x in test if x in train] # all are unique\n",
    "\n",
    "# CV file\n",
    "cv_acr_1 = {}\n",
    "cv_acr_1[\"run_1\"] = {\"test\" : p_data.loc[p_data.type == \"test\", ].index.to_list(),\n",
    "                     \"train\" : p_data.loc[p_data.type == \"train\", ].index.to_list()}\n",
    "\n",
    "# Write as json\n",
    "write_json(cv_acr_1, \"/proj/inputs/acr_cv.json\") # saves the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3ca64a-e3f9-4ae1-91fa-f55c6306d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "g_data = pd.read_feather(\"/proj/ext_dir/data_ravi/14_10_2024/example_geno.feather\")\n",
    "g_data = g_data.set_index('genotype')\n",
    "all_geno = train + test\n",
    "\n",
    "# Check calls for heterozygotes\n",
    "#for col in g_data:\n",
    "#    if(int(1) in g_data[col].unique()):\n",
    "#        print(col)\n",
    "\n",
    "# Check if all phenotype records have associated genotypic data\n",
    "[print(x)for x in g_data.index.to_list() if x not in all_geno] # all are there\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def scale_data(to_transform, pd_cols, pd_index):\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    data_scaled = scaler.fit_transform(to_transform)\n",
    "    data_scaled_df = pd.DataFrame(data_scaled, columns = pd_cols, index = pd_index)\n",
    "    return data_scaled_df, scaler\n",
    "\n",
    "g_data_mod, acr_g_scl = scale_data(g_data, g_data.columns, g_data.index)\n",
    "p_data_mod, acr_p_scl = scale_data(p_data.loc[:, \"blups\"].values.reshape(-1, 1), \n",
    "                                   [\"blups\"],\n",
    "                                   p_data.index)\n",
    "\n",
    "acr_p = p_data.merge(p_data_mod, how='left', left_index=True, right_index=True, \n",
    "                         sort=False, suffixes=('_raw', '_scaled'))\n",
    "\n",
    "# reshape geno data to fit the p_data\n",
    "np_g_data = np.stack([g_data_mod[g_data_mod.index == idx].iloc[0,:].values for idx in acr_p['genotype']])\n",
    "acr_g = np_g_data.reshape(np_g_data.shape[0], np_g_data.shape[1], 1)\n",
    "\n",
    "# check shape\n",
    "# acr_g.shape\n",
    "\n",
    "# save data\n",
    "import pickle\n",
    "\n",
    "def write_pkl(data, path, verbose = False):\n",
    "    with open(path, \"wb\") as fp:   # pickling\n",
    "        pickle.dump(data, fp)\n",
    "    if verbose:\n",
    "        return print(\"Done\")\n",
    "    \n",
    "out_paths = {}\n",
    "out_paths['acr_g.npy'] = \"/proj/inputs/acr_g.npy\"\n",
    "out_paths['acr_p.csv'] = \"/proj/inputs/acr_p.csv\"\n",
    "out_paths['acr_g.scl'] = \"/proj/inputs/acr_g.scl\"\n",
    "out_paths['acr_p.scl'] = \"/proj/inputs/acr_p.scl\"\n",
    "\n",
    "np.save(out_paths['acr_g.npy'], acr_g)\n",
    "acr_p.to_csv(out_paths['acr_p.csv'], index=False)\n",
    "write_pkl(acr_g_scl, out_paths['acr_g.scl'])\n",
    "write_pkl(acr_p_scl, out_paths['acr_p.scl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b4ee0d-86bd-431f-8a33-dda783dc5916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the run scripts\n",
    "# cd /proj/run/Py\n",
    "# doit\n",
    "\n",
    "# run the analysis\n",
    "# acquire a gpu cluster\n",
    "# run /qg-10/data/AGR-QG/Gogna/tutorials/tut_CNN/outputs/create_slurm_scripts/acr_CNN_acr_cv/master_script.sh create by previous step\n",
    "\n",
    "# import predictions\n",
    "import pandas as pd\n",
    "output = pd.read_csv(\"/proj/outputs/create_slurm_scripts/acr_CNN_acr_cv/run_1/pred/output.csv\") \n",
    "\n",
    "# check prediction ability\n",
    "output['obs'].corr(output['pred']).round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
